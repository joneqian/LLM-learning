{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_D9kG_efts3"
   },
   "source": [
    "# Transformers 模型量化技术：AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE9IhcVyktah"
   },
   "source": [
    "![img](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/Thumbnail.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwsg6nCwoThm"
   },
   "source": [
    "在2023年6月，Ji Lin等人发表了论文[AWQ：Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/pdf/2306.00978.pdf)。\n",
    "\n",
    "这篇论文详细介绍了一种激活感知权重量化算法，可以用于压缩任何基于 Transformer 的语言模型，同时只有微小的性能下降。关于 AWQ 算法的详细介绍，见[MIT Han Song 教授分享](https://hanlab.mit.edu/projects/awq)。\n",
    "\n",
    "transformers 现在支持两个不同的 AWQ 开源实现库：\n",
    "\n",
    "- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)\n",
    "- [LLM-AWQ](https://github.com/mit-han-lab/llm-awq) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H2019RkoiM-"
   },
   "source": [
    "因为 LLM-AWQ 不支持 Nvidia T4 GPU（课程演示 GPU），所以我们使用 AutoAWQ 库来介绍和演示 AWQ 模型量化技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化前模型测试文本生成任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\admin\\anaconda3\\envs\\transformer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\admin\\anaconda3\\envs\\transformer\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:59<00:00, 29.98s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"facebook/opt-6.7b\"\n",
    "\n",
    "# 使用 GPU 加载原始的 OPT-125m 模型\n",
    "generator = pipeline('text-generation',\n",
    "                     model=model_path,\n",
    "                     device=0,\n",
    "                     do_sample=True,\n",
    "                     num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实测GPU显存占用：加载 OPT-125m 模型后\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 15:11:33 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   47C    P0              26W /  70W |    635MiB / 15360MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The woman worked as a housewoman in the office. After that, she moved to the shop as'},\n",
       " {'generated_text': 'The woman worked as a salesperson at the store and was said to display the utmost professionalism in her'},\n",
       " {'generated_text': 'The woman worked as a nurse there in 2018. He was fired from his job at the hospital on'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The woman worked as a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The man worked as a bus driver.  A job that requires you to interact with a large number'},\n",
       " {'generated_text': 'The man worked as a contractor at a property for years, and was recently fired when he reported it'},\n",
       " {'generated_text': 'The man worked as a security guard last night. He must be in trouble.\\nThere’'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The man worked as a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dJJRQ2p7eLQ"
   },
   "source": [
    "## 使用 AutoAWQ 量化模型\n",
    "\n",
    "下面我们以 `facebook opt-125m` 模型为例，使用 `AutoAWQ` 库实现的 AWQ 算法实现模型量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\admin\\anaconda3\\envs\\transformer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\admin\\anaconda3\\envs\\transformer\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files:   0%|                                                                        | 0/12 [00:00<?, ?it/s]D:\\Users\\admin\\anaconda3\\envs\\transformer\\Lib\\site-packages\\huggingface_hub-0.20.1-py3.8.egg\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\.cache\\huggingface\\models--facebook--opt-6.7b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "Fetching 12 files: 100%|███████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 14.90it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"facebook/opt-6.7b\"\n",
    "quant_path = \"models/opt-6.7b-awq\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "\n",
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Qn_P_E5p7gAN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 167B [00:00, ?B/s] \n",
      "D:\\Users\\admin\\anaconda3\\envs\\transformer\\Lib\\site-packages\\huggingface_hub-0.20.1-py3.8.egg\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data files:   0%|                                                                    | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                     | 0.00/471M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|▌                                                           | 4.19M/471M [00:03<06:06, 1.27MB/s]\u001b[A\n",
      "Downloading data:   3%|█▌                                                          | 12.6M/471M [00:05<03:09, 2.42MB/s]\u001b[A\n",
      "Downloading data:   4%|██▋                                                         | 21.0M/471M [00:08<02:34, 2.91MB/s]\u001b[A\n",
      "Downloading data:   6%|███▋                                                        | 29.4M/471M [00:10<02:26, 3.02MB/s]\u001b[A\n",
      "Downloading data:   8%|████▊                                                       | 37.7M/471M [00:12<02:13, 3.25MB/s]\u001b[A\n",
      "Downloading data:  10%|█████▉                                                      | 46.1M/471M [00:15<02:05, 3.37MB/s]\u001b[A\n",
      "Downloading data:  12%|██████▉                                                     | 54.5M/471M [00:17<02:00, 3.45MB/s]\u001b[A\n",
      "Downloading data:  13%|████████                                                    | 62.9M/471M [00:21<02:15, 3.01MB/s]\u001b[A\n",
      "Downloading data:  15%|█████████                                                   | 71.3M/471M [00:23<02:07, 3.13MB/s]\u001b[A\n",
      "Downloading data:  17%|██████████▏                                                 | 79.7M/471M [00:25<01:59, 3.28MB/s]\u001b[A\n",
      "Downloading data:  19%|███████████▏                                                | 88.1M/471M [00:28<01:53, 3.36MB/s]\u001b[A\n",
      "Downloading data:  20%|████████████▎                                               | 96.5M/471M [00:30<01:49, 3.42MB/s]\u001b[A\n",
      "Downloading data:  22%|█████████████▌                                               | 105M/471M [00:33<01:48, 3.38MB/s]\u001b[A\n",
      "Downloading data:  24%|██████████████▋                                              | 113M/471M [00:35<01:45, 3.39MB/s]\u001b[A\n",
      "Downloading data:  26%|███████████████▊                                             | 122M/471M [00:38<01:44, 3.36MB/s]\u001b[A\n",
      "Downloading data:  28%|████████████████▊                                            | 130M/471M [00:40<01:42, 3.34MB/s]\u001b[A\n",
      "Downloading data:  29%|█████████████████▉                                           | 138M/471M [00:42<01:37, 3.39MB/s]\u001b[A\n",
      "Downloading data:  31%|███████████████████                                          | 147M/471M [00:45<01:35, 3.41MB/s]\u001b[A\n",
      "Downloading data:  33%|████████████████████                                         | 155M/471M [00:47<01:31, 3.45MB/s]\u001b[A\n",
      "Downloading data:  35%|█████████████████████▏                                       | 164M/471M [00:50<01:27, 3.49MB/s]\u001b[A\n",
      "Downloading data:  37%|██████████████████████▎                                      | 172M/471M [00:51<01:18, 3.82MB/s]\u001b[A\n",
      "Downloading data:  38%|███████████████████████▎                                     | 180M/471M [00:54<01:17, 3.74MB/s]\u001b[A\n",
      "Downloading data:  40%|████████████████████████▍                                    | 189M/471M [00:56<01:17, 3.66MB/s]\u001b[A\n",
      "Downloading data:  42%|█████████████████████████▌                                   | 197M/471M [00:58<01:15, 3.64MB/s]\u001b[A\n",
      "Downloading data:  44%|██████████████████████████▌                                  | 206M/471M [01:01<01:13, 3.61MB/s]\u001b[A\n",
      "Downloading data:  45%|███████████████████████████▋                                 | 214M/471M [01:02<01:04, 4.01MB/s]\u001b[A\n",
      "Downloading data:  47%|████████████████████████████▊                                | 222M/471M [01:05<01:04, 3.87MB/s]\u001b[A\n",
      "Downloading data:  49%|█████████████████████████████▉                               | 231M/471M [01:06<00:58, 4.13MB/s]\u001b[A\n",
      "Downloading data:  51%|██████████████████████████████▉                              | 239M/471M [01:09<00:58, 3.96MB/s]\u001b[A\n",
      "Downloading data:  53%|████████████████████████████████                             | 247M/471M [01:11<00:58, 3.84MB/s]\u001b[A\n",
      "Downloading data:  54%|█████████████████████████████████▏                           | 256M/471M [01:13<00:57, 3.77MB/s]\u001b[A\n",
      "Downloading data:  56%|██████████████████████████████████▏                          | 264M/471M [01:16<00:55, 3.73MB/s]\u001b[A\n",
      "Downloading data:  58%|███████████████████████████████████▎                         | 273M/471M [01:18<00:55, 3.55MB/s]\u001b[A\n",
      "Downloading data:  60%|████████████████████████████████████▍                        | 281M/471M [01:21<00:53, 3.57MB/s]\u001b[A\n",
      "Downloading data:  61%|█████████████████████████████████████▍                       | 289M/471M [01:23<00:52, 3.45MB/s]\u001b[A\n",
      "Downloading data:  63%|██████████████████████████████████████▌                      | 298M/471M [01:26<00:49, 3.50MB/s]\u001b[A\n",
      "Downloading data:  65%|███████████████████████████████████████▋                     | 306M/471M [01:28<00:47, 3.46MB/s]\u001b[A\n",
      "Downloading data:  67%|████████████████████████████████████████▋                    | 315M/471M [01:30<00:40, 3.87MB/s]\u001b[A\n",
      "Downloading data:  69%|█████████████████████████████████████████▊                   | 323M/471M [01:32<00:38, 3.83MB/s]\u001b[A\n",
      "Downloading data:  70%|██████████████████████████████████████████▉                  | 331M/471M [01:33<00:33, 4.17MB/s]\u001b[A\n",
      "Downloading data:  72%|████████████████████████████████████████████                 | 340M/471M [01:36<00:34, 3.83MB/s]\u001b[A\n",
      "Downloading data:  74%|█████████████████████████████████████████████                | 348M/471M [01:38<00:29, 4.20MB/s]\u001b[A\n",
      "Downloading data:  76%|██████████████████████████████████████████████▏              | 357M/471M [01:40<00:28, 3.97MB/s]\u001b[A\n",
      "Downloading data:  77%|███████████████████████████████████████████████▎             | 365M/471M [01:42<00:27, 3.85MB/s]\u001b[A\n",
      "Downloading data:  79%|████████████████████████████████████████████████▎            | 373M/471M [01:45<00:25, 3.80MB/s]\u001b[A\n",
      "Downloading data:  81%|█████████████████████████████████████████████████▍           | 382M/471M [01:47<00:22, 3.95MB/s]\u001b[A\n",
      "Downloading data:  83%|██████████████████████████████████████████████████▌          | 390M/471M [01:49<00:20, 3.87MB/s]\u001b[A\n",
      "Downloading data:  85%|███████████████████████████████████████████████████▌         | 398M/471M [01:50<00:17, 4.21MB/s]\u001b[A\n",
      "Downloading data:  86%|████████████████████████████████████████████████████▋        | 407M/471M [01:53<00:16, 3.98MB/s]\u001b[A\n",
      "Downloading data:  88%|█████████████████████████████████████████████████████▊       | 415M/471M [01:55<00:14, 3.87MB/s]\u001b[A\n",
      "Downloading data:  90%|██████████████████████████████████████████████████████▉      | 424M/471M [01:58<00:13, 3.56MB/s]\u001b[A\n",
      "Downloading data:  92%|███████████████████████████████████████████████████████▉     | 432M/471M [02:00<00:10, 3.55MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████████████████████████████████████████████████████    | 440M/471M [02:03<00:08, 3.46MB/s]\u001b[A\n",
      "Downloading data:  95%|██████████████████████████████████████████████████████████▏  | 449M/471M [02:04<00:05, 3.82MB/s]\u001b[A\n",
      "Downloading data:  97%|███████████████████████████████████████████████████████████▏ | 457M/471M [02:07<00:03, 3.73MB/s]\u001b[A\n",
      "Downloading data:  99%|████████████████████████████████████████████████████████████▎| 466M/471M [02:09<00:01, 3.69MB/s]\u001b[A\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████| 471M/471M [02:11<00:00, 3.59MB/s]\u001b[A\n",
      "Downloading data files: 100%|███████████████████████████████████████████████████████████| 1/1 [02:11<00:00, 131.21s/it]\n",
      "Extracting data files: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "Generating validation split: 214670 examples [00:02, 71779.84 examples/s]\n",
      "AWQ: 100%|██████████████████████████████████████████████████████████████████████████| 32/32 [1:09:21<00:00, 130.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实测GPU显存使用：量化模型时峰值达到将近 4GB\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 15:12:50 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   48C    P0              32W /  70W |    3703MiB / 15360MiB |      2%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nVzKDBlP_6MV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuPLq9sa8EaN"
   },
   "source": [
    "#### Transformers 兼容性配置\n",
    "\n",
    "为了使`quant_config` 与 transformers 兼容，我们需要修改配置文件：`使用 Transformers.AwqConfig 来实例化量化模型配置`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KE8xjwlL8DnA"
   },
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/opt-6.7b-awq\\\\tokenizer_config.json',\n",
       " 'models/opt-6.7b-awq\\\\special_tokens_map.json',\n",
       " 'models/opt-6.7b-awq\\\\vocab.json',\n",
       " 'models/opt-6.7b-awq\\\\merges.txt',\n",
       " 'models/opt-6.7b-awq\\\\added_tokens.json',\n",
       " 'models/opt-6.7b-awq\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)  # 保存分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 GPU 加载量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"cuda\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't worry.I will help you to get rid of this problem.\n",
      "\n",
      "I have a very good experience in this field.I have done many projects in this field.I am sure that I will do this project perfectly.\n",
      "\n",
      "I am a professional web developer. I have more than 5 years of experience in web development. I have worked on many\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Don't worry.I will help you to\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z0hAXYanCDW3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The girl worked as a waitress at a restaurant in the city of Krasnoyarsk, in the Urals region of Russia.\n",
      "\n",
      "The man, who was not named, was a regular customer at the restaurant.\n",
      "\n",
      "He was a regular customer at the restaurant.\n",
      "\n",
      "He was a regular customer at the restaurant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The girl worked as a\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework：使用 AWQ 量化 Facebook OPT-6.7B 模型\n",
    "\n",
    "Facebook OPT 模型：https://huggingface.co/facebook?search_models=opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
